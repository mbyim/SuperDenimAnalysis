
# coding: utf-8

# In[1]:

import scrapy
from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from superdenim.items import PostsItem
from superdenim.items import ThreadsItem


# In[3]:

#Write the Spider


#Creating "start_urls" list
urllist = []
for i in range(0,119):
    url = "http://supertalk.superfuture.com/index.php/forum/22-superdenim/" + "page-" + str(i)
    url = url.strip('"')
    #page_url_list.append(url)
    urllist.append(url)

class denimspider(CrawlSpider):
    #Spyder to grab Thread info and URLs
    #Will then use the resulting information to build a list of thread URL's to use for the PostSpyder
    name = 'denimspider'
    download_delay = 2
    start_urls = urllist
    
    def parse(self, response):
        #Parses each page of the list of threads
        allthreadurls = []
        items = []
        for thread in response.xpath('//tr[@class="__topic  expandable"]'):
            item = ThreadsItem()
            thread_url= thread.xpath('.//a[@itemprop="url"]/@href').extract()
            allthreadurls.append(thread_url)
            item['threadurl'] = thread_url
            item['threadcreator'] = thread.xpath('.//span[@class="desc lighter blend_links"]//span[@itemprop="name"]/text()').extract()
            item['threaddate'] = thread.xpath('.//span[@class="desc lighter blend_links"]//span[@itemprop="dateCreated"]/text()').extract()
            #does this indexing on extract work properly (in shell it did...)
            item['thread_replies'] = thread.xpath('.//td[@class="col_f_views desc blend_links"]/ul/li/text()').extract()[1]
            item['thread_views'] = thread.xpath('.//td[@class="col_f_views desc blend_links"]/ul/li[@class="views desc"]/text()').extract()
            yield item
            
        for url in allthreadurls:
            print "in parse function"
            url = url[0]
            print url
            print type(url)
            yield scrapy.Request(url, self.parse_thread)
            

    
    def parse_thread(self, response):
        items = []
        for post in response.xpath('//div[@class="post_block hentry clear clearfix  "]'):
            item = PostsItem()
            item['posturl'] = post.xpath('.//a[@itemprop="replyToUrl"]/@href').extract()
            item['postnum'] = post.xpath('.//a[@itemprop="replyToUrl"]/text()').extract()[0]
            item['posterID'] = post.xpath('.//div[@class="author_info"]//span[@itemprop="name"]/text()').extract()
            item['posterClass'] = post.xpath('.//div[@class="author_info"]//li[@class="group_title"]/span/text()').extract()
            #all xpaths above this comment in this script should be gudd
            item['poster_post_count'] = post.xpath('.//div[@class="author_info"]//li[@class="post_count desc lighter"]/text()').extract()[0]
            #trying w/o this to see if the errors generated by poster_join indexing error actually lead to cutting off 500,000 posts...
            #item['poster_join'] = post.xpath('.//div[@class="author_info"]//li[@class="post_count desc lighter"]/text()').extract()[1]
            item['postdate'] = post.xpath('.//div[@class="post_body"]//abbr[@itemprop="commentTime"]/text()').extract()
            #all xpaths above this comment in this script should be gudd
            item['postcontent'] = post.xpath('.//div[@class="post_body"]//div[@itemprop="commentText"]/text()').extract()
            
            yield item
        
        next_page = response.xpath('//a[@rel="next"]/@href')
        if next_page:
            url = str(response.urljoin(next_page[0].extract())).strip('"')
            print "in parse_thread function"
            yield scrapy.Request(url, self.parse_thread)
        

